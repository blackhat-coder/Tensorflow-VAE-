{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.84765531e+10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([20000.0999833434]) * np.array([923823.034930])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.function_base import gradient\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "from numpy.lib.function_base import gradient\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from utilss import LATENT_DIM\n",
    "from utilss import NO_EPOCHS\n",
    "from utilss import BATCH_SIZE\n",
    "from utilss import LEARNING_RATE\n",
    "from utilss import LOG_DIR\n",
    "\n",
    "from models.model_def import VAE\n",
    "from models.model_def import Encoder\n",
    "from models.model_def import Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LATENT_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_test(keras.Model):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Encoder_test, self).__init__(**kwargs)\n",
    "\n",
    "        # self.input_layer = keras.layers.Input(shape=(INPUT_SIZE))\n",
    "\n",
    "        # self.f = keras.layers.Flatten()\n",
    "        # self.l1 = keras.layers.Dense(364, activation=\"relu\")\n",
    "        # self.l2 = keras.layers.Dense(224, activation=\"relu\")\n",
    "        # self.l3 = keras.layers.Dense(124, activation=\"relu\")\n",
    "        # self.l4 = keras.layers.Dense(64, activation=\"relu\")\n",
    "        # self.l5 = keras.layers.Dense(24)\n",
    "                \n",
    "        self.conv_layer_one = keras.layers.Conv2D(24, 3, padding=\"valid\", activation=\"relu\") # 26 x 26\n",
    "        self.conv_layer_two = keras.layers.Conv2D(64, 3, padding=\"valid\", activation=\"relu\") # 24 X 24\n",
    "        self.pool_one = keras.layers.MaxPool2D(2, 2) # 12 x 12 X 64\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "\n",
    "        # inputs = self.input_layer(inputs)\n",
    "        # inputs = self.f(inputs)\n",
    "        # inputs = self.l1(inputs)\n",
    "        # inputs = self.l2(inputs)\n",
    "        # inputs = self.l3(inputs)\n",
    "        # inputs = self.l4(inputs)\n",
    "        # inputs = self.l5(inputs)\n",
    "\n",
    "\n",
    "        inputs = self.conv_layer_one(inputs)\n",
    "        inputs = self.conv_layer_two(inputs)\n",
    "        inputs = self.pool_one(inputs)\n",
    "\n",
    "        flatten = keras.layers.Flatten()(inputs)\n",
    "        fcn = keras.layers.Dense(64, activation=\"relu\")(flatten)\n",
    "\n",
    "        mean = keras.layers.Dense(LATENT_DIM)(fcn)\n",
    "        mu_ = keras.layers.Dense(LATENT_DIM)(fcn)\n",
    "\n",
    "        # mean = keras.layers.Dense(LATENT_DIM)(inputs)\n",
    "        # mu_ = keras.layers.Dense(LATENT_DIM)(inputs)\n",
    "\n",
    "        # enc_model = keras.models.Model( \n",
    "        #     inputs=[inputs], \n",
    "        #     outputs=[mean, mu_]\n",
    "        # )\n",
    "        \n",
    "        return (mean, mu_)\n",
    "\n",
    "class Decoder_test(keras.Model):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder_test, self).__init__(**kwargs)\n",
    "\n",
    "        self.input_layer = keras.layers.Input(shape=(LATENT_DIM) )\n",
    "        # self.dense_layer = keras.layers.Dense(7 * 7 * 128)\n",
    "        # self.reshape_layer = keras.layers.Reshape((7, 7, 128))\n",
    "\n",
    "        # self.l_1 = keras.layers.Dense(24, activation=\"relu\")\n",
    "        # self.l_2 = keras.layers.Dense(64, activation=\"relu\")\n",
    "        # self.l_3 = keras.layers.Dense(124, activation=\"relu\")\n",
    "        # self.l_4 = keras.layers.Dense(224, activation=\"relu\")\n",
    "        # self.l_5 = keras.layers.Dense(364, activation=\"relu\")\n",
    "        # self.l_6 = keras.layers.Dense(28*28, activation=\"sigmoid\")\n",
    "\n",
    "        self.l_1_c = keras.layers.Dense(12*12, activation=\"relu\")\n",
    "        self.l_2_c = keras.layers.Reshape((12, 12, 1))\n",
    "        self.l_3_c = keras.layers.Conv2DTranspose(64, 3, padding=\"valid\", activation=\"relu\")\n",
    "        self.l_4_c = keras.layers.Conv2DTranspose(24, 3, strides=2,padding=\"same\", activation=\"relu\")\n",
    "        self.l_5_c = keras.layers.Conv2DTranspose(1, 3, padding=\"same\", activation=\"sigmoid\")\n",
    "        self.r = keras.layers.Reshape((28, 28))\n",
    "\n",
    "        # write the sequential model in __call__()\n",
    "    def __call__(self, inputs):\n",
    "        \n",
    "        dec_model = keras.models.Sequential([\n",
    "            self.input_layer,\n",
    "            # self.l_1,\n",
    "            self.l_1_c,\n",
    "            self.l_2_c,\n",
    "            self.l_3_c,\n",
    "            self.l_4_c,\n",
    "            self.l_5_c,\n",
    "            self.r\n",
    "        ])\n",
    "\n",
    "        return dec_model(inputs)\n",
    "\n",
    "\n",
    "class Sampling(keras.layers.Layer):\n",
    "    \n",
    "    '''\n",
    "    Reparamaterization Trick\n",
    "    '''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Sampling, self).__init__(**kwargs)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "\n",
    "        '''\n",
    "        Args:\n",
    "            inputs - A tuple containing (mean, variance)\n",
    "            output - A vector of shape LATENT_DIM\n",
    "        '''\n",
    "\n",
    "        mean, log_var = inputs\n",
    "        sample = tf.random.normal([LATENT_DIM]) * tf.exp(log_var / 2) + mean\n",
    "\n",
    "        return sample\n",
    "\n",
    "class VAE_test(keras.Model):\n",
    "    # Implement Holding the latent Vector\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(VAE_test, self).__init__(**kwargs)\n",
    "\n",
    "        self.encoder = Encoder_test()\n",
    "        self.decoder = Decoder_test()\n",
    "    \n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        '''\n",
    "        Args:\n",
    "            -input : inputs , shape = (batch, INPUT_SIZE , INPUT_SIZE)\n",
    "\n",
    "            -output : (reconstruction, mean, log_var)\n",
    "                reconstruction : shape = (batch, INPUT_SIZE, INPUT_SIZE)\n",
    "                mean           : shape = (LATENT_DIM, 1)\n",
    "                log_var        : shape = (LATENT_DIM, 1)\n",
    "    \n",
    "        '''\n",
    "\n",
    "        inputs = inputs\n",
    "\n",
    "        mean, log_var = self.encoder(inputs) #shape : INPUT_SIZE\n",
    "\n",
    "        latent_vector = Sampling()((mean, log_var))\n",
    "\n",
    "        reconstruction = self.decoder(latent_vector)\n",
    "\n",
    "        # vae_model = keras.models.Model(inputs=[inputs], outputs=[reconstruction])\n",
    "\n",
    "        return reconstruction, mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch == 20:\n",
    "        lr = epoch / 10\n",
    "    if epoch == 30:\n",
    "        lr = epoch / 10\n",
    "    \n",
    "    return lr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keras_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "\n",
    "# Refactor Network_loss into class Network_loss\n",
    "def Network_loss(mean, log_var, alpha=1, beta=1):\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.reshape(y_true, (-1, 28, 28))\n",
    "        y_pred = tf.reshape(y_pred, (-1, 28, 28))\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "        # assert y_true.dtype = tf.float32, \"use tf.cast() to cast y_true to type tf.float32\"\n",
    "\n",
    "        latent_loss = 1 + log_var - tf.square(mean) - tf.exp(log_var)\n",
    "        latent_loss = -0.5 + tf.reduce_sum(latent_loss, 1)\n",
    "\n",
    "        recons_loss = y_true * tf.math.log(1e-10+y_pred) + (1 - y_true) * tf.math.log(1e-10 + 1 - y_pred)\n",
    "        recons_loss = -tf.reduce_sum(recons_loss)\n",
    "\n",
    "        network_loss = alpha * recons_loss + beta * latent_loss\n",
    "        network_loss = tf.reduce_mean(network_loss)\n",
    "\n",
    "        return network_loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
    "x_test = np.reshape(x_test, (-1, 28, 28, 1))\n",
    "\n",
    "x_train = x_train/255.0\n",
    "\n",
    "\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "x_train = x_train[4:204]\n",
    "y_train = y_train[4:204]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(15)\n",
    "\n",
    "\n",
    "p_t = []\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.005)\n",
    "\n",
    "log_grads = []\n",
    "\n",
    "def LogsWriter():\n",
    "    log_dir_name = datetime.datetime.now().strftime(\"%d%m%y-%H%M%S\")\n",
    "\n",
    "    train_writer = tf.summary.create_file_writer(f\"{LOG_DIR}\\\\{log_dir_name}\\\\train\")\n",
    "    val_writer = tf.summary.create_file_writer(f\"{LOG_DIR}\\\\{log_dir_name}\\\\val\")\n",
    "\n",
    "    return train_writer, val_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({9: 23, 2: 20, 1: 27, 3: 22, 4: 20, 5: 12, 6: 19, 7: 21, 8: 16, 0: 20})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "counter = Counter\n",
    "counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a9c384c820>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.reshape(x_train[6], (28, 28, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tarining.py\n",
      "WARNING:tensorflow:Layer conv2d_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch: 0\n",
      "loss: 1929.931884765625\n",
      "Epoch: 1\n",
      "loss: 1884.4320068359375\n",
      "Epoch: 2\n",
      "loss: 1328.06982421875\n",
      "Epoch: 3\n",
      "loss: 1073.9688720703125\n",
      "Epoch: 4\n",
      "loss: 1167.7239990234375\n",
      "Epoch: 5\n",
      "loss: 1059.5389404296875\n",
      "Epoch: 6\n",
      "loss: 1130.499267578125\n",
      "Epoch: 7\n",
      "loss: 1114.3753662109375\n",
      "Epoch: 8\n",
      "loss: 1239.11962890625\n",
      "Epoch: 9\n",
      "loss: 954.0924072265625\n",
      "Epoch: 10\n",
      "loss: 1000.9608154296875\n",
      "Epoch: 11\n",
      "loss: 1052.642578125\n",
      "Epoch: 12\n",
      "loss: 963.2615356445312\n",
      "Epoch: 13\n",
      "loss: 1228.86572265625\n",
      "Epoch: 14\n",
      "loss: 1167.2259521484375\n",
      "Epoch: 15\n",
      "loss: 927.2703247070312\n",
      "Epoch: 16\n",
      "loss: 1066.1822509765625\n",
      "Epoch: 17\n",
      "loss: 1010.1853637695312\n",
      "Epoch: 18\n",
      "loss: 938.8693237304688\n",
      "Epoch: 19\n",
      "loss: 1218.3255615234375\n",
      "Epoch: 20\n",
      "loss: 1092.254150390625\n",
      "Epoch: 21\n",
      "loss: 884.5733642578125\n",
      "Epoch: 22\n",
      "loss: 935.3818359375\n",
      "Epoch: 23\n",
      "loss: 893.8424072265625\n",
      "Epoch: 24\n",
      "loss: 1033.0048828125\n",
      "Epoch: 25\n",
      "loss: 1139.6043701171875\n",
      "Epoch: 26\n",
      "loss: 899.1932373046875\n",
      "Epoch: 27\n",
      "loss: 1033.7532958984375\n",
      "Epoch: 28\n",
      "loss: 1127.3182373046875\n",
      "Epoch: 29\n",
      "loss: 1102.4072265625\n",
      "session ended\n"
     ]
    }
   ],
   "source": [
    "def train(train_writer=True):\n",
    "\n",
    "    model = VAE()\n",
    "\n",
    "    if train_writer == True:\n",
    "        t_writer, v_writer = LogsWriter()\n",
    "\n",
    "    for epoch in range(30):\n",
    "\n",
    "        for step, (batch_x, batch_y) in enumerate(train_dataset):\n",
    "            # print(batch_x.shape)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                (predictions, mean, lg_var) = model(batch_x)\n",
    "\n",
    "                loss = Network_loss(mean, lg_var, alpha=1, beta=1)\n",
    "                loss_ = loss(batch_x, predictions)\n",
    "\n",
    "                # batch_x = np.reshape(batch_x, (1,784))\n",
    "                # loss_ = keras_loss(batch_x, predictions)\n",
    "            \n",
    "            gradients = tape.gradient(loss_, model.trainable_weights)\n",
    "            \n",
    "            clipped_grads = [tf.clip_by_value(grad, -1., 1.)  for grad in gradients]\n",
    "\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "        # Logging loss\n",
    "        with t_writer.as_default():\n",
    "            tf.summary.scalar(\"train_loss\", loss_, step=epoch)\n",
    "\n",
    "            t_writer.flush()\n",
    "\n",
    "        log_grads.append({ f\"epoch:{epoch}\":[ {\"max\":max} , {\"min\": min}, {\"mean\":mean}   ] })\n",
    "\n",
    "        if epoch > 30:\n",
    "            optimizer._initial_decay = 2.0\n",
    "            \n",
    "        print(f\"Epoch: {epoch}\\nloss: {loss_}\")\n",
    "        p_t.append(predictions)\n",
    "        # Write Checkpoint at epoch level or at batch level\n",
    "\n",
    "\n",
    "# def test_loss():\n",
    "#     import matplotlib as mpl\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     import cv2\n",
    "#     x_t = x_train[4:5]\n",
    "\n",
    "#     t_model = VAE()\n",
    "#     t_y, m, v = t_model(x_t)\n",
    "\n",
    "#     print(\"pred shape\")\n",
    "#     print(t_y.shape)\n",
    "#     print(\"input shape\")\n",
    "#     print(x_t.shape)\n",
    "\n",
    "#     t_y = np.reshape(t_y, [28, 28])\n",
    "#     x_t = np.reshape(x_t, [28, 28])\n",
    "    \n",
    "#     cv2.imwrite(\"x_t.jpg\", x_t)\n",
    "#     # cv2.imwrite(\"t_y.jpg\", t_y)\n",
    "    \n",
    "#     print(f\"loss:{ Network_loss(m, v)(x_t, t_y) } \")\n",
    "\n",
    "#     # print(np.array(x_t))\n",
    "#     plt.imshow(x_t)\n",
    "#     # plt.imshow(t_y)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting tarining.py\")\n",
    "    train()\n",
    "    # test_loss()~\n",
    "    print(\"session ended\")\n",
    "    # print(\"hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer = adam(lr=0.001), batch_size = 7, 100 samples, average loss at epoch 10 - 330\n",
    "optimizer = adam(lr=0.001), batch_size = 10, 200 samples, average loss at epoch 3 - 2000\n",
    "optimizer = adam(lr=0.001), batch_size = 10, 200 samples, average loss at epoch 4 - 2347\n",
    "optimizer = adam(lr=0.01), batch_size = 20, 200 samples, loss at first epoch - 9000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!. I'm Excited!!! We have a lead, with optimizer Adam, loss:174, lr:0.001\n",
    "\n",
    "batch size- 5, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([5, 28, 28])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_t[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a9e8484520>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(np.reshape(p_t[29][0], [28, 28])) \n",
    "plt.imshow(np.reshape(p_t[29][4], [28, 28]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!. I'm Excited!!! We have a lead, with optimizer Adam, loss:174, lr:0.001.\n",
    "\n",
    "with Adam VAE(), latent_dim = 4, loss = 120(36 epoch), lr=0.003, batch_size=5, trian_len=46, epochs=50, we can see something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adam.Adam at 0x1e0a62d9a00>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer._initial_decay = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7188dd80f53663500f2258cb10eb1832a6ea2b437bc9bbd5dff078eab8e6fa4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('condaenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
